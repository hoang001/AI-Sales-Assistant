import requests
from bs4 import BeautifulSoup
import json
import re
import time
import random
import os
from urllib.parse import urljoin

# --- C·∫§U H√åNH ---
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept-Language': 'vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7'
}


# --- H√ÄM H·ªñ TR·ª¢ (UTILS) ---
def clean_text(text):
    """L√†m s·∫°ch vƒÉn b·∫£n: x√≥a kho·∫£ng tr·∫Øng th·ª´a, xu·ªëng d√≤ng"""
    if not text: return ""
    return re.sub(r'\s+', ' ', text).strip()


def parse_price(price_str):
    """
    Chuy·ªÉn ƒë·ªïi chu·ªói gi√° '29.990.000‚Ç´' th√†nh s·ªë nguy√™n 29990000.
    H√†m n√†y ƒë∆∞·ª£c c·∫£i ti·∫øn ƒë·ªÉ x·ª≠ l√Ω nhi·ªÅu ƒë·ªãnh d·∫°ng h∆°n.
    """
    if not price_str: return 0

    # ∆Øu ti√™n t√¨m chu·ªói s·ªë c√≥ d·∫•u ch·∫•m ngƒÉn c√°ch (v√≠ d·ª•: 29.990.000)
    match = re.search(r'(\d{1,3}(?:\.\d{3})+)', price_str)
    if match:
        clean_str = re.sub(r'[^\d]', '', match.group(1))
        if clean_str:
            return int(clean_str)

    # N·∫øu kh√¥ng c√≥, th·ª≠ c√°ch ƒë∆°n gi·∫£n l√† x√≥a h·∫øt k√Ω t·ª± kh√¥ng ph·∫£i s·ªë
    clean_str = re.sub(r'[^\d]', '', price_str)
    try:
        price = int(clean_str)
        # Ch·ªâ ch·∫•p nh·∫≠n gi√° tr·ªã l·ªõn h∆°n 100.000 ƒë·ªÉ tr√°nh nh·∫ßm l·∫´n v·ªõi c√°c s·ªë kh√°c
        if price > 100000:
            return price
        return 0
    except (ValueError, TypeError):
        return 0


def extract_specs_dict(soup):
    """L·∫•y th√¥ng s·ªë k·ªπ thu·∫≠t d∆∞·ªõi d·∫°ng dictionary key-value t·ª´ nhi·ªÅu c·∫•u tr√∫c web"""
    specs_dict = {}

    # Ph∆∞∆°ng ph√°p cho CellphoneS
    tech_items = soup.select('.technical-content-item')
    if tech_items:
        for item in tech_items:
            title = item.select_one('.technical-content-item__title')
            content = item.select_one('.technical-content-item__content')
            if title and content:
                key = clean_text(title.text)
                value = clean_text(content.text)
                if key and value:
                    specs_dict[key] = value

    # Ph∆∞∆°ng ph√°p d·ª± ph√≤ng chung (b·∫£ng)
    if not specs_dict:
        tables = soup.find_all('table')
        for table in tables:
            rows = table.find_all('tr')
            for row in rows:
                cols = row.find_all(['th', 'td'])
                if len(cols) == 2:
                    key = clean_text(cols[0].text)
                    value = clean_text(cols[1].text)
                    if key and value:
                        specs_dict[key] = value

    return specs_dict


# --- LOGIC CRAWLER ---

def crawl_category(category_url):
    """
    Thu th·∫≠p t·∫•t c·∫£ c√°c link s·∫£n ph·∫©m t·ª´ m·ªôt trang danh m·ª•c.
    :param category_url: URL c·ªßa trang danh m·ª•c
    :return: M·ªôt danh s√°ch c√°c URL s·∫£n ph·∫©m ƒë·∫ßy ƒë·ªß.
    """
    print(f"üîé ƒêang qu√©t trang danh m·ª•c: {category_url}...")
    product_links = []
    try:
        response = requests.get(category_url, headers=HEADERS, timeout=15)
        if response.status_code != 200:
            print(f"‚ö†Ô∏è L·ªói {response.status_code} khi qu√©t danh m·ª•c {category_url}")
            return []

        soup = BeautifulSoup(response.content, 'lxml')
        links = soup.select('div.product-info a.product__link')

        if not links:
            print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y link s·∫£n ph·∫©m n√†o v·ªõi c√°c selector ƒë√£ bi·∫øt.")

        for link in links:
            href = link.get('href')
            if href and not href.startswith('http'):
                href = urljoin(category_url, href)
            if href:
                product_links.append(href)

        unique_links = sorted(list(set(product_links)))
        print(f"‚úÖ T√¨m th·∫•y {len(unique_links)} link s·∫£n ph·∫©m.")
        return unique_links

    except Exception as e:
        print(f"‚ö†Ô∏è Ngo·∫°i l·ªá khi qu√©t danh m·ª•c: {e}")
        return []


def crawl_product(url):
    """
    Thu th·∫≠p th√¥ng tin t·ª´ m·ªôt URL s·∫£n ph·∫©m.
    H√†m n√†y c√≥ kh·∫£ nƒÉng t√°ch c√°c phi√™n b·∫£n s·∫£n ph·∫©m (VD: 256GB, 512GB)
    t·ª´ c√πng m·ªôt trang v√† l·ªçc b·ªè c√°c s·∫£n ph·∫©m kh√¥ng li√™n quan.
    :param url: URL c·ªßa trang s·∫£n ph·∫©m
    :return: M·ªôt danh s√°ch c√°c dictionary s·∫£n ph·∫©m.
    """
    print(f"üîÑ ƒêang x·ª≠ l√Ω: {url}...")
    try:
        time.sleep(random.uniform(1, 2))
        response = requests.get(url, headers=HEADERS, timeout=15)
        if response.status_code != 200:
            print(f"‚ö†Ô∏è L·ªói {response.status_code} khi truy c·∫≠p {url}")
            return []

        soup = BeautifulSoup(response.content, 'lxml')

        # 1. L·∫•y th√¥ng tin chung v√† x√°c ƒë·ªãnh T√äN S·∫¢N PH·∫®M C·ªêT L√ïI
        name_tag = soup.find('h1')
        base_name = clean_text(name_tag.text) if name_tag else "Kh√¥ng t√™n"

        # Lo·∫°i b·ªè dung l∆∞·ª£ng v√† c√°c th√¥ng tin ph·ª• ƒë·ªÉ c√≥ t√™n g·ªëc
        core_name = re.sub(r'\s*(\d+\s*(GB|TB)|\|.*$)', '', base_name, flags=re.IGNORECASE).strip()
        main_product_identifier = core_name

        specs_dict = extract_specs_dict(soup)

        desc_text = ""
        desc_div = soup.find('div', class_='card-content') or soup.find('div', class_='ksp-content')
        if desc_div:
            desc_text = clean_text(desc_div.get_text(separator=' ', strip=True))
        if len(desc_text) < 50:
            desc_text = ""

        category = "ƒêi·ªán tho·∫°i"
        name_lower = base_name.lower()
        url_lower = url.lower()
        laptop_keywords = ["laptop", "macbook", "strix", "rog", "vivobook", "zenbook"]
        tablet_keywords = ["tablet", "ipad", "tab"]

        if any(keyword in url_lower or keyword in name_lower for keyword in laptop_keywords):
            category = "Laptop"
        elif any(keyword in url_lower or keyword in name_lower for keyword in tablet_keywords):
            category = "Tablet"

        product_variations = []
        keys_to_remove_from_specs = []

        # 2. T√°ch c√°c phi√™n b·∫£n s·∫£n ph·∫©m
        # C√ÅCH 1: T·ª´ b·∫£ng th√¥ng s·ªë (cho s·∫£n ph·∫©m s·∫Øp ra m·∫Øt)
        variations_dict = {}
        for key, value in specs_dict.items():
            price_from_value = parse_price(value)
            if main_product_identifier.lower() in key.lower() and price_from_value > 0:
                normalized_name = clean_text(key)
                normalized_name = re.sub(r'^(Gi√°|Gi√° d·ª± ki·∫øn|Phi√™n b·∫£n)\s+', '', normalized_name, flags=re.IGNORECASE).strip()
                normalized_name = re.sub(r'\s*\|.*$', '', normalized_name).strip()

                if normalized_name not in variations_dict:
                    variations_dict[normalized_name] = {
                        "name": normalized_name,
                        "price_int": price_from_value,
                    }
                keys_to_remove_from_specs.append(key)

        if variations_dict:
            product_variations = list(variations_dict.values())

        # C√ÅCH 2: T·ª´ box ch·ªçn phi√™n b·∫£n (cho s·∫£n ph·∫©m th√¥ng th∆∞·ªùng)
        if not product_variations:
            variation_container = soup.find('div', class_='box-content-group')
            if variation_container:
                items = variation_container.find_all('a', class_='item-child')
                for item in items:
                    name_div = item.find('div', class_='name')
                    price_p = item.find('p', class_='special-price')
                    if name_div and price_p:
                        variation_name = clean_text(name_div.text)
                        if main_product_identifier.lower() in variation_name.lower():
                            variation_price = parse_price(price_p.text)
                            if variation_price > 0:
                                product_variations.append({
                                    "name": variation_name,
                                    "price_int": variation_price,
                                })

        # 3. T·∫°o danh s√°ch s·∫£n ph·∫©m cu·ªëi c√πng
        final_products = []
        common_specs = specs_dict.copy()
        for key in set(keys_to_remove_from_specs):
            if key in common_specs:
                del common_specs[key]

        if product_variations:
            print(f"‚úÖ T√¨m th·∫•y {len(product_variations)} phi√™n b·∫£n cho: {core_name}")
            for variation in product_variations:
                variation_specs = common_specs.copy()
                storage_match = re.search(r'(\d+\s*GB|\d+\s*TB)', variation["name"], re.IGNORECASE)
                if storage_match:
                    variation_specs["B·ªô nh·ªõ trong"] = storage_match.group(1).strip()

                final_products.append({
                    "url": url,
                    "name": variation["name"],
                    "price_int": variation["price_int"],
                    "category": category,
                    "specs": variation_specs,
                    "rag_content": f"S·∫£n ph·∫©m: {variation['name']}. Gi√° b√°n kho·∫£ng: {variation['price_int']:,} ƒë·ªìng. C·∫•u h√¨nh chi ti·∫øt: {json.dumps(variation_specs, ensure_ascii=False)}. T√≠nh nƒÉng n·ªïi b·∫≠t: {desc_text}"
                })
        else:
            # Fallback: N·∫øu kh√¥ng c√≥ phi√™n b·∫£n, l·∫•y gi√° ch√≠nh c·ªßa trang
            price_int = 0
            price_label = soup.find('div', class_='price-label')
            if price_label:
                next_element = price_label.find_next_sibling()
                if next_element:
                    price_int = parse_price(next_element.text)

            if price_int == 0:
                price_selectors = [
                    '.box-info__box-price .product__price--show',
                    '.product-price-current',
                    '.special-price',
                ]
                for selector in price_selectors:
                    price_tag = soup.select_one(selector)
                    if price_tag:
                        price_int = parse_price(price_tag.text)
                        if price_int > 0:
                            break

            if price_int > 0:
                print(f"‚úÖ Th√†nh c√¥ng (1 s·∫£n ph·∫©m): {base_name} - {price_int:,} ƒë")
            else:
                print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y gi√° cho: {base_name}. Th√™m s·∫£n ph·∫©m v·ªõi gi√° 0.")

            final_products.append({
                "url": url,
                "name": base_name,
                "price_int": price_int,
                "category": category,
                "specs": common_specs,
                "rag_content": f"S·∫£n ph·∫©m: {base_name}. Gi√° b√°n kho·∫£ng: {price_int:,} ƒë·ªìng. C·∫•u h√¨nh chi ti·∫øt: {json.dumps(common_specs, ensure_ascii=False)}. T√≠nh nƒÉng n·ªïi b·∫≠t: {desc_text}"
            })

        return final_products

    except Exception as e:
        print(f"‚ö†Ô∏è Ngo·∫°i l·ªá khi x·ª≠ l√Ω {url}: {e}")
        return []


# --- MAIN EXECUTION ---
if __name__ == "__main__":
    category_urls = [
        "https://cellphones.com.vn/tablet.html",
    ]

    all_product_links = []
    print("üöÄ B·∫Øt ƒë·∫ßu qu√° tr√¨nh kh√°m ph√° link s·∫£n ph·∫©m...")
    for cat_url in category_urls:
        links = crawl_category(cat_url)
        if links:
            all_product_links.extend(links)

    unique_product_links = sorted(list(set(all_product_links)))
    print(f"\n‚û°Ô∏è T·ªïng c·ªông s·∫Ω thu th·∫≠p d·ªØ li·ªáu t·ª´ {len(unique_product_links)} s·∫£n ph·∫©m.")

    crawled_data = []
    print("\nüöÄ B·∫Øt ƒë·∫ßu qu√° tr√¨nh thu th·∫≠p d·ªØ li·ªáu chi ti·∫øt...")

    for link in unique_product_links:
        products = crawl_product(link)
        if products:
            crawled_data.extend(products)

    # --- B∆Ø·ªöC KH·ª¨ TR√ôNG L·∫∂P ---
    print("\nüîÑ B·∫Øt ƒë·∫ßu qu√° tr√¨nh kh·ª≠ tr√πng l·∫∑p s·∫£n ph·∫©m...")
    final_unique_products = []
    seen_product_names = set()
    for product in crawled_data:
        product_name = product.get("name", "")
        # Chu·∫©n h√≥a t√™n ƒë·ªÉ x·ª≠ l√Ω c√°c h·∫≠u t·ªë nh∆∞ "| Ch√≠nh h√£ng"
        normalized_name = re.sub(r'\s*\|.*$', '', product_name).strip()
        if normalized_name and normalized_name not in seen_product_names:
            final_unique_products.append(product)
            seen_product_names.add(normalized_name)

    if len(crawled_data) > len(final_unique_products):
        print(f"‚úÖ ƒê√£ lo·∫°i b·ªè {len(crawled_data) - len(final_unique_products)} s·∫£n ph·∫©m tr√πng l·∫∑p.")

    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    output_dir = os.path.join(base_dir, 'data', 'raw')
    os.makedirs(output_dir, exist_ok=True)
    output_file = os.path.join(output_dir, 'products.json')

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(final_unique_products, f, ensure_ascii=False, indent=2)

    print(f"\nüéâ Ho√†n t·∫•t! D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i:\n{output_file}")
    print(f"T·ªïng s·ªë s·∫£n ph·∫©m ƒë√£ thu th·∫≠p: {len(final_unique_products)}")
