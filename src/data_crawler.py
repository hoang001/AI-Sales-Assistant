import requests
from bs4 import BeautifulSoup
import json
import re
import time
import random
import os
from urllib.parse import urljoin

# --- C·∫§U H√åNH ---
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept-Language': 'vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7'
}


# --- H√ÄM H·ªñ TR·ª¢ (UTILS) ---
def clean_text(text):
    """L√†m s·∫°ch vƒÉn b·∫£n: x√≥a kho·∫£ng tr·∫Øng th·ª´a, xu·ªëng d√≤ng"""
    if not text: return ""
    return re.sub(r'\s+', ' ', text).strip()


def parse_price(price_str):
    """
    Chuy·ªÉn ƒë·ªïi chu·ªói gi√° '29.990.000‚Ç´' th√†nh s·ªë nguy√™n 29990000.
    H√†m n√†y ƒë∆∞·ª£c c·∫£i ti·∫øn ƒë·ªÉ x·ª≠ l√Ω nhi·ªÅu ƒë·ªãnh d·∫°ng h∆°n.
    """
    if not price_str: return 0

    # ∆Øu ti√™n t√¨m chu·ªói s·ªë c√≥ d·∫•u ch·∫•m ngƒÉn c√°ch (v√≠ d·ª•: 29.990.000)
    match = re.search(r'(\d{1,3}(?:\.\d{3})+)', price_str)
    if match:
        clean_str = re.sub(r'[^\d]', '', match.group(1))
        if clean_str:
            return int(clean_str)

    # N·∫øu kh√¥ng c√≥, th·ª≠ c√°ch ƒë∆°n gi·∫£n l√† x√≥a h·∫øt k√Ω t·ª± kh√¥ng ph·∫£i s·ªë
    clean_str = re.sub(r'[^\d]', '', price_str)
    try:
        price = int(clean_str)
        # Ch·ªâ ch·∫•p nh·∫≠n gi√° tr·ªã l·ªõn h∆°n 100.000 ƒë·ªÉ tr√°nh nh·∫ßm l·∫´n v·ªõi c√°c s·ªë kh√°c
        if price > 100000:
            return price
        return 0
    except (ValueError, TypeError):
        return 0


def extract_specs_dict(soup):
    """L·∫•y th√¥ng s·ªë k·ªπ thu·∫≠t d∆∞·ªõi d·∫°ng dictionary key-value t·ª´ nhi·ªÅu c·∫•u tr√∫c web"""
    specs_dict = {}

    # Ph∆∞∆°ng ph√°p cho CellphoneS
    if not specs_dict:
        tech_items = soup.select('.technical-content-item')
        if tech_items:
            for item in tech_items:
                title = item.select_one('.technical-content-item__title')
                content = item.select_one('.technical-content-item__content')
                if title and content:
                    key = clean_text(title.text)
                    value = clean_text(content.text)
                    if key and value:
                        specs_dict[key] = value

    # Ph∆∞∆°ng ph√°p d·ª± ph√≤ng chung (b·∫£ng)
    if not specs_dict:
        tables = soup.find_all('table')
        for table in tables:
            rows = table.find_all('tr')
            for row in rows:
                cols = row.find_all(['th', 'td'])
                if len(cols) == 2:
                    key = clean_text(cols[0].text)
                    value = clean_text(cols[1].text)
                    if key and value:
                        specs_dict[key] = value

    return specs_dict


# --- LOGIC CRAWLER ---

def crawl_category(category_url):
    """
    Thu th·∫≠p t·∫•t c·∫£ c√°c link s·∫£n ph·∫©m t·ª´ m·ªôt trang danh m·ª•c.
    :param category_url: URL c·ªßa trang danh m·ª•c
    :return: M·ªôt danh s√°ch c√°c URL s·∫£n ph·∫©m ƒë·∫ßy ƒë·ªß.
    """
    print(f"üîé ƒêang qu√©t trang danh m·ª•c: {category_url}...")
    product_links = []
    try:
        response = requests.get(category_url, headers=HEADERS, timeout=15)
        if response.status_code != 200:
            print(f"‚ö†Ô∏è L·ªói {response.status_code} khi qu√©t danh m·ª•c {category_url}")
            return []

        soup = BeautifulSoup(response.content, 'lxml')
        links = []

        # S·ª≠ d·ª•ng selector ph√π h·ª£p cho t·ª´ng trang
        if "cellphones.com.vn" in category_url:
            links = soup.select('div.product-info a.product__link')

        if not links:
            print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y link s·∫£n ph·∫©m n√†o v·ªõi c√°c selector ƒë√£ bi·∫øt.")

        for link in links:
            href = link.get('href')
            if href and not href.startswith('http'):
                href = urljoin(category_url, href)
            if href:
                product_links.append(href)

        unique_links = sorted(list(set(product_links)))
        print(f"‚úÖ T√¨m th·∫•y {len(unique_links)} link s·∫£n ph·∫©m.")
        return unique_links

    except Exception as e:
        print(f"‚ö†Ô∏è Ngo·∫°i l·ªá khi qu√©t danh m·ª•c: {e}")
        return []


def crawl_product(url):
    """
    Thu th·∫≠p th√¥ng tin t·ª´ m·ªôt URL s·∫£n ph·∫©m.
    H√†m n√†y c√≥ kh·∫£ nƒÉng t√°ch c√°c phi√™n b·∫£n s·∫£n ph·∫©m (VD: 256GB, 512GB)
    t·ª´ c√πng m·ªôt trang v√† l·ªçc b·ªè c√°c s·∫£n ph·∫©m kh√¥ng li√™n quan.
    :param url: URL c·ªßa trang s·∫£n ph·∫©m
    :return: M·ªôt danh s√°ch c√°c dictionary s·∫£n ph·∫©m.
    """
    print(f"üîÑ ƒêang x·ª≠ l√Ω: {url}...")
    try:
        time.sleep(random.uniform(1, 2))
        response = requests.get(url, headers=HEADERS, timeout=15)
        if response.status_code != 200:
            print(f"‚ö†Ô∏è L·ªói {response.status_code} khi truy c·∫≠p {url}")
            return []

        soup = BeautifulSoup(response.content, 'lxml')

        # 1. L·∫•y th√¥ng tin chung v√† x√°c ƒë·ªãnh T√äN S·∫¢N PH·∫®M C·ªêT L√ïI
        name_tag = soup.find('h1')
        base_name = clean_text(name_tag.text) if name_tag else "Kh√¥ng t√™n"

        # Lo·∫°i b·ªè dung l∆∞·ª£ng v√† c√°c th√¥ng tin ph·ª• ƒë·ªÉ c√≥ t√™n g·ªëc
        core_name = re.sub(r'\s*(\d+\s*(GB|TB)|\|.*$)', '', base_name, flags=re.IGNORECASE).strip()
        main_product_identifier = core_name

        specs_dict = extract_specs_dict(soup)

        desc_text = ""
        desc_div = soup.find('div', class_='card-content') or soup.find('div', class_='ksp-content')
        if desc_div:
            desc_text = clean_text(desc_div.get_text(separator=' ', strip=True))
        if len(desc_text) < 50:
            desc_text = ""

        category = "ƒêi·ªán tho·∫°i"
        name_lower = base_name.lower()
        url_lower = url.lower()
        laptop_keywords = ["laptop", "macbook", "strix", "rog", "vivobook", "zenbook"]
        if any(keyword in url_lower or keyword in name_lower for keyword in laptop_keywords):
            category = "Laptop"

        product_variations = []
        keys_to_remove_from_specs = []

        # 2. T√°ch c√°c phi√™n b·∫£n s·∫£n ph·∫©m
        if not product_variations:
            variation_container = soup.find('div', class_='box-content-group')
            if variation_container:
                items = variation_container.find_all('a', class_='item-child')
                for item in items:
                    name_div = item.find('div', class_='name')
                    price_p = item.find('p', class_='special-price')
                    if name_div and price_p:
                        variation_name = clean_text(name_div.text)
                        if main_product_identifier.lower() in variation_name.lower():
                            product_variations.append({
                                "name": variation_name,
                                "price_int": parse_price(price_p.text),
                            })

        # 3. T·∫°o danh s√°ch s·∫£n ph·∫©m cu·ªëi c√πng
        final_products = []
        common_specs = specs_dict.copy()
        for key in set(keys_to_remove_from_specs):
            if key in common_specs:
                del common_specs[key]

        if product_variations:
            print(f"‚úÖ T√¨m th·∫•y {len(product_variations)} phi√™n b·∫£n cho: {core_name}")
            for variation in product_variations:
                variation_specs = common_specs.copy()
                storage_match = re.search(r'(\d+\s*GB|\d+\s*TB)', variation["name"], re.IGNORECASE)
                if storage_match:
                    variation_specs["B·ªô nh·ªõ trong"] = storage_match.group(1).strip()

                final_products.append({
                    "url": url,
                    "name": variation["name"],
                    "price_int": variation["price_int"],
                    "category": category,
                    "specs": variation_specs,
                    "rag_content": f"S·∫£n ph·∫©m: {variation['name']}. Gi√° b√°n kho·∫£ng: {variation['price_int']:,} ƒë·ªìng. C·∫•u h√¨nh chi ti·∫øt: {json.dumps(variation_specs, ensure_ascii=False)}. T√≠nh nƒÉng n·ªïi b·∫≠t: {desc_text}"
                })
        else:
            # Fallback: N·∫øu kh√¥ng c√≥ phi√™n b·∫£n, l·∫•y gi√° ch√≠nh c·ªßa trang
            price_int = 0

            # C√ÅCH 1: D·ª±a v√†o g·ª£i √Ω c·ªßa b·∫°n - t√¨m label v√† th·∫ª gi√° ƒëi li·ªÅn k·ªÅ
            price_label = soup.find('div', class_='price-label')
            if price_label:
                next_element = price_label.find_next_sibling()
                if next_element:
                    price_int = parse_price(next_element.text)

            # C√ÅCH 2: N·∫øu c√°ch tr√™n kh√¥ng th√†nh c√¥ng, th·ª≠ c√°c selector CSS ph·ªï bi·∫øn
            if price_int == 0:
                price_selectors = [
                    '.box-info__box-price .product__price--show', # CellphoneS
                    '.product-price-current',                    # CellphoneS (d·ª± ph√≤ng)
                    '.special-price',                            # CellphoneS (khuy·∫øn m√£i)
                    '.style-price-special',                      # FPT Shop (khuy·∫øn m√£i)
                    '.style-price',                              # FPT Shop (gi√° g·ªëc)
                ]
                for selector in price_selectors:
                    price_tag = soup.select_one(selector)
                    if price_tag:
                        price_int = parse_price(price_tag.text)
                        if price_int > 0:
                            break

            if price_int > 0:
                print(f"‚úÖ Th√†nh c√¥ng (1 s·∫£n ph·∫©m): {base_name} - {price_int:,} ƒë")
            else:
                print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y gi√° cho: {base_name}. Th√™m s·∫£n ph·∫©m v·ªõi gi√° 0.")

            final_products.append({
                "url": url,
                "name": base_name,
                "price_int": price_int,
                "category": category,
                "specs": common_specs,
                "rag_content": f"S·∫£n ph·∫©m: {base_name}. Gi√° b√°n kho·∫£ng: {price_int:,} ƒë·ªìng. C·∫•u h√¨nh chi ti·∫øt: {json.dumps(common_specs, ensure_ascii=False)}. T√≠nh nƒÉng n·ªïi b·∫≠t: {desc_text}"
            })

        return final_products

    except Exception as e:
        print(f"‚ö†Ô∏è Ngo·∫°i l·ªá khi x·ª≠ l√Ω {url}: {e}")
        return []


# --- MAIN EXECUTION ---
if __name__ == "__main__":
    # URL m·ª•c ti√™u l√† c√°c trang danh m·ª•c s·∫£n ph·∫©m
    category_urls = [
        "https://cellphones.com.vn/laptop/asus.html",

    ]

    all_product_links = []
    print("üöÄ B·∫Øt ƒë·∫ßu qu√° tr√¨nh kh√°m ph√° link s·∫£n ph·∫©m...")
    for cat_url in category_urls:
        links = crawl_category(cat_url)
        if links:
            all_product_links.extend(links)

    unique_product_links = sorted(list(set(all_product_links)))
    print(f"\n‚û°Ô∏è T·ªïng c·ªông s·∫Ω thu th·∫≠p d·ªØ li·ªáu t·ª´ {len(unique_product_links)} s·∫£n ph·∫©m.")

    crawled_data = []
    print("\nüöÄ B·∫Øt ƒë·∫ßu qu√° tr√¨nh thu th·∫≠p d·ªØ li·ªáu chi ti·∫øt...")

    for link in unique_product_links:
        products = crawl_product(link)
        if products:
            crawled_data.extend(products)

    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    output_dir = os.path.join(base_dir, 'data', 'raw')
    os.makedirs(output_dir, exist_ok=True)
    output_file = os.path.join(output_dir, 'products.json')

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(crawled_data, f, ensure_ascii=False, indent=2)

    print(f"\nüéâ Ho√†n t·∫•t! D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i:\n{output_file}")
    print(f"T·ªïng s·ªë s·∫£n ph·∫©m ƒë√£ thu th·∫≠p: {len(crawled_data)}")
